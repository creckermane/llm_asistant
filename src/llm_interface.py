# src/llm_interface.py
import requests
import logging
from src.config import OLLAMA_BASE_URL, OLLAMA_MODEL

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –º–æ–¥—É–ª—è
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


class OllamaLLM:
    """
    –ö–ª–∞—Å—Å –¥–ª—è –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è —Å –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é Ollama.
    """

    def __init__(self, model_name: str):
        self.model_name = model_name
        self.base_url = OLLAMA_BASE_URL
        logging.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OllamaLLM —Å –º–æ–¥–µ–ª—å—é '{self.model_name}' –ø–æ URL '{self.base_url}'")

    def generate(self, prompt: str, temperature: float = 0.0) -> str:
        """
        –û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–æ–º–ø—Ç –≤ –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Ollama –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ—Ç–≤–µ—Ç.
        """
        return self._call_ollama(prompt, temperature)

    def _call_ollama(self, prompt: str, temperature: float) -> str:
        """
        –í—ã–ø–æ–ª–Ω—è–µ—Ç –∑–∞–ø—Ä–æ—Å –∫ –ª–æ–∫–∞–ª—å–Ω–æ–º—É Ollama API –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ç–≤–µ—Ç–∞ –æ—Ç LLM.
        """
        try:
            logging.info(f"üì® –û—Ç–ø—Ä–∞–≤–ª—è—é –≤ Ollama (–º–æ–¥–µ–ª—å: {self.model_name}, temp: {temperature}): {prompt[:100]}...")

            response = requests.post(
                f"{self.base_url}/api/generate",
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature
                    }
                },
                timeout=180
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º HTTP-—Å—Ç–∞—Ç—É—Å –æ—Ç–≤–µ—Ç–∞
            if response.status_code != 200:
                logging.error(f"‚ùå Ollama –≤–µ—Ä–Ω—É–ª —Å—Ç–∞—Ç—É—Å {response.status_code}: {response.text}")
                return "[–û—à–∏–±–∫–∞: –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç –º–æ–¥–µ–ª–∏ Ollama]"

            data = response.json()

            if "response" not in data:
                logging.warning(f"‚ö†Ô∏è –ù–µ—Ç –ø–æ–ª—è 'response' –≤ –æ—Ç–≤–µ—Ç–µ Ollama: {data}")
                return "[–û—à–∏–±–∫–∞: –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç LLM Ollama]"

            raw_text = data["response"].strip()

            # –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ —à—É–º/–º—É—Å–æ—Ä –æ—Ç –º–æ–¥–µ–ª–∏
            clean_text = raw_text.split("</")[0].strip() if "</" in raw_text else raw_text
            clean_text = clean_text.replace("Assistant:", "").strip()

            logging.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {clean_text[:100]}...")
            return clean_text

        except requests.exceptions.ConnectionError:
            logging.error(
                "üî¥ –û—à–∏–±–∫–∞ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è: –Ω–µ –º–æ–≥—É –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ ollama –∑–∞–ø—É—â–µ–Ω ('ollama serve').")
            return "[–û—à–∏–±–∫–∞: –Ω–µ —É–¥–∞–µ—Ç—Å—è –ø–æ–¥–∫–ª—é—á–∏—Ç—å—Å—è –∫ Ollama. –ó–∞–ø—É—Å—Ç–∏—Ç–µ 'ollama serve'?]"
        except requests.exceptions.Timeout:
            logging.error(
                "‚è∞ –¢–∞–π–º–∞—É—Ç (180—Å) –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ Ollama. –ú–æ–¥–µ–ª—å —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω–∞—è –∏–ª–∏ –∑–∞–ø—Ä–æ—Å —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π.")
            return "[–û—à–∏–±–∫–∞: —Ç–∞–π–º–∞—É—Ç –æ—Ç–≤–µ—Ç–∞ –æ—Ç –º–æ–¥–µ–ª–∏ Ollama]"
        except Exception as e:
            logging.exception(f"üî¥ –ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≤—ã–∑–æ–≤–µ Ollama: {e}")
            return f"[–í–Ω—É—Ç—Ä–µ–Ω–Ω—è—è –æ—à–∏–±–∫–∞ Ollama: {str(e)}]"


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥—É–ª—è (–¥–ª—è –∞–≤—Ç–æ–Ω–æ–º–Ω–æ–≥–æ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è)
if __name__ == "__main__":
    print("--- –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ LLM Interface (Ollama) ---")


    llm_test_instance = OllamaLLM(model_name="gemma3:1b")  # –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∞—à—É –º–æ–¥–µ–ª—å

    test_prompt_simple = "–ü—Ä–∏–≤–µ—Ç, –∫–∞–∫ –¥–µ–ª–∞? –û—Ç–≤–µ—Ç—å –∫—Ä–∞—Ç–∫–æ."
    print(f"\n–í–æ–ø—Ä–æ—Å: {test_prompt_simple}")
    response_simple = llm_test_instance.generate(test_prompt_simple)
    print(f"–û—Ç–≤–µ—Ç LLM: {response_simple}")

    test_prompt_complex = "–†–∞—Å—Å–∫–∞–∂–∏ –æ —Å–µ–±–µ, –∫–∞–∫ –æ —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏."
    print(f"\n–í–æ–ø—Ä–æ—Å: {test_prompt_complex}")
    response_complex = llm_test_instance.generate(test_prompt_complex)
    print(f"–û—Ç–≤–µ—Ç LLM: {response_complex}")
